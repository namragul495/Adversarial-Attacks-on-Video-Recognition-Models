{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5133d9ec-1b81-4589-b19f-f26effc165c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "orignal =[] #Blank array. But later on add normal and abnormal to it. original has all the labels we got during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b5dc95e3-1d01-4802-aae9-67cd1d5c3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all test videos\n",
    "test=[] #It contains all labels (0 and 1). Length is 85 \n",
    "test_list2=[]\n",
    "f= 'sample_images_RA'\n",
    "Test_videos =[]  #This array contains path of all videos.Length is 85\n",
    "test_list = open('Labels/test.txt','r') # path and labels info \n",
    "for line in test_list:\n",
    "    name = line.split(\" \")[0]  \n",
    "    test_list2.append(line)\n",
    "    x=line.split(\" \")[-1]\n",
    "    test.append(x[0])\n",
    "    Test_videos.append(f+\"/\"+name) #Full path of all videos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8d979bd6-a028-446f-8064-38311c43203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test: \n",
    "    if i=='0':\n",
    "        orignal.append('normal')\n",
    "    else:\n",
    "        orignal.append('abnormal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a91d574e-85c1-4c8c-a278-8f62d415aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load pre-trained model, trained on 5 classes \n",
    "#model = tf.keras.models.load_model('Trained models/model_c3d.h5',compile=False)\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "#To load pre-trained weeights,\n",
    "model = tf.keras.models.load_model('Trained Models/weights_model_C3D.hdf5',compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8041b922-4f68-4679-b123-558095e05419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Five class labels\n",
    "labels1 = {0: 'fighting', 1: 'vandalism',2: '0normal',3: 'shooting',4: 'hockeyfight'}\n",
    "prediction = [] #Labels you get after testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "302eb5ec-128c-4409-9b83-5669f049319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_contrast(frame, alpha, beta):\n",
    "    contrasted_frame = cv2.convertScaleAbs(frame, alpha=3.0, beta=255)\n",
    "    return contrasted_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "122c2f74-d959-4813-99a8-25ebaf83ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Confusion Matrix\n",
    "for video in Test_videos:\n",
    "    vid = [] #reads frame of videos\n",
    "    #nor=0  #no value of nor. initialize nor with 0\n",
    "    abnor=0   #no value of abnor. initialize abnor 0\n",
    "    path=video\n",
    "    images = os.listdir(path) #Images name actually store name of all frames\n",
    "    for img in images: #read the length and width of all images.\n",
    "        img_path=path +\"/\"+img #path +img name print. path of 1 frame\n",
    "        img2 = cv2.imread(img_path) #img2 has a 2D array of integer form containing length and width of videos\n",
    "        img2 = adjust_contrast(img2, alpha=3.0, beta=255)\n",
    "        img2= cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        vid.append(cv2.resize(img2, (112, 112))) \n",
    "    vid = np.array(vid, dtype=np.float32) #model accepts float32 type of array from training code. (no of frames, length, width)\n",
    "    for i in range(0,len(vid),32): #len is actually no of frames.\n",
    "        X = vid[i:i+32].transpose((2,1,0,3)) #model takes 32 frames.\n",
    "        output = model.predict_on_batch(np.array([X]))\n",
    "        if abnor==3:\n",
    "            break\n",
    "        if(len(output)>0): #code doesnot break\n",
    "            str1=labels1[np.argmax(output)] #on which location of labels is the max probablity of o/p\n",
    "            if str1=='vandalism' or str1 == 'fighting' or str1=='shooting' or str1=='hockeyfight':\n",
    "                abnor=abnor+1\n",
    "            else:\n",
    "                abnor=0\n",
    "    \n",
    "    vid=[]\n",
    "    if abnor==3:\n",
    "         prediction.append('abnormal')\n",
    "    else:\n",
    "        prediction.append('normal')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "76f7d2c5-6ba4-4145-b2ff-2594ef982e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction) #prediction array has labels we got during prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "90bc2501-b25c-49a5-a788-1be31177c0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(orignal, prediction)\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9686280-7e02-440d-b05d-076891810bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
