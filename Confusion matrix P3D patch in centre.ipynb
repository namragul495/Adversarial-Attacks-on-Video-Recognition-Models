{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13218aa2-62ca-448f-978b-b558a2a55722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "orignal =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9de66f82-c500-46cd-af26-bd0538a98cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all test videos\n",
    "test=[]\n",
    "f= 'sample_images_RA'\n",
    "Test_videos =[]\n",
    "test_list = open('Labels/test.txt','r')\n",
    "for line in test_list:\n",
    "    name = line.split(\" \")[0]\n",
    "    x=line.split(\" \")[-1]\n",
    "    test.append(x[0])\n",
    "    Test_videos.append(f+\"/\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2210a82-cfce-45c2-b4c5-66c6bef8cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    if i=='0':\n",
    "        orignal.append('normal')\n",
    "    else:\n",
    "        orignal.append('abnormal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17d9852e-da91-4975-8b90-7398c12d39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load pre-trained model, trained on 5 classes \n",
    "#model = tf.keras.models.load_model('Trained Models/model_P3D.h5',compile=False)\n",
    "\n",
    "\n",
    "#To load pre-trained weights,\n",
    "model = tf.keras.models.load_model('Trained Models/weights_model_P3D.hdf5',compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f934831f-c2be-4499-96c9-b3f5990ce38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two classes labels\n",
    "labels0 ={0:'normal', 1:'abnormal'}\n",
    "prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fde5184-0502-445c-b6d9-79874476de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Patch image\n",
    "wm = cv2.imread(\"patch 2.jpg\")\n",
    "wm = cv2.resize(wm, (25, 25))\n",
    "wm =cv2.cvtColor(wm, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b5c5e02-4633-4f42-a487-d5916dea2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have defined 'Test_videos', 'wm', 'labels0', and 'model' elsewhere in your code\n",
    "\n",
    "prediction = []\n",
    "\n",
    "for video in Test_videos:\n",
    "    vid = []\n",
    "    nor = 0\n",
    "    abnor = 0\n",
    "    path = video\n",
    "    images = os.listdir(video)\n",
    "    \n",
    "    for img in images:\n",
    "        img_path = os.path.join(path, img)\n",
    "        img2 = cv2.imread(img_path)\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        img2 = cv2.resize(img2, (112, 112))\n",
    "        \n",
    "        # Extracting the central 25x25 patch\n",
    "        middle_patch = img2[43:68, 43:68]  # Adjust the indices based on your image size\n",
    "        \n",
    "        result = cv2.addWeighted(middle_patch, 1, wm, 1, 0)\n",
    "        img2[43:68, 43:68] = result\n",
    "        vid.append(img2)\n",
    "    \n",
    "    vid = np.array(vid, dtype=np.float32)\n",
    "    length = len(vid)\n",
    "    \n",
    "    for i in range(0, len(vid), 32):\n",
    "        if abnor == 3:\n",
    "            break\n",
    "        if i + 32 <= length:\n",
    "            X = vid[i:i + 32].transpose((1, 2, 0, 3))\n",
    "            output = model.predict_on_batch(np.array([X]))\n",
    "            \n",
    "            if len(output) > 0:\n",
    "                str1 = labels0[np.argmax(output)]\n",
    "                \n",
    "                if str1 == 'abnormal':\n",
    "                    abnor += 1\n",
    "                else:\n",
    "                    abnor = 0\n",
    "    \n",
    "    vid = []\n",
    "    \n",
    "    if abnor == 3:\n",
    "        prediction.append('abnormal')\n",
    "    else:\n",
    "        prediction.append('normal')\n",
    "\n",
    "# Now 'prediction' contains the predictions for each video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d46d6e4-9a43-40be-949e-3ea81da36e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have defined 'Test_videos', 'wm', 'labels0', and 'model' elsewhere in your code\n",
    "\n",
    "prediction = []\n",
    "\n",
    "for video in Test_videos:\n",
    "    vid = []\n",
    "    nor = 0\n",
    "    abnor = 0\n",
    "    path = video\n",
    "    images = os.listdir(video)\n",
    "    \n",
    "    for img in images:\n",
    "        img_path = os.path.join(path, img)\n",
    "        img2 = cv2.imread(img_path)\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        img2 = cv2.resize(img2, (112, 112))\n",
    "        \n",
    "        # Extracting the central 25x25 patch\n",
    "        middle_patch = img2[43:68, 43:68]  # Adjust the indices based on your image size\n",
    "        \n",
    "        result = cv2.addWeighted(middle_patch, 1, wm, 1, 0)\n",
    "        img2[43:68, 43:68] = result\n",
    "        vid.append(img2)\n",
    "    \n",
    "    vid = np.array(vid, dtype=np.float32)\n",
    "    length = len(vid)\n",
    "    \n",
    "    for i in range(0, len(vid), 32):\n",
    "        if abnor == 3:\n",
    "            break\n",
    "        if i + 32 <= length:\n",
    "            X = vid[i:i + 32].transpose((1, 2, 0, 3))\n",
    "            output = model.predict_on_batch(np.array([X]))\n",
    "            \n",
    "            if len(output) > 0:\n",
    "                str1 = labels0[np.argmax(output)]\n",
    "                \n",
    "                if str1 == 'abnormal':\n",
    "                    abnor += 1\n",
    "                else:\n",
    "                    abnor = 0\n",
    "    \n",
    "    vid = []\n",
    "    \n",
    "    if abnor == 3:\n",
    "        prediction.append('abnormal')\n",
    "    else:\n",
    "        prediction.append('normal')\n",
    "\n",
    "# Now 'prediction' contains the predictions for each video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd70f526-7e38-4e2c-8a98-63a1c3eddb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33efa50d-2140-4b57-90bb-cdfa786ed0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7799054c-440a-4f5a-8997-200c830f83ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_images_RA/0Normal/1Fighting037_x264 normal normal\n",
      "sample_images_RA/0Normal/1Fighting038_x264 normal normal\n",
      "sample_images_RA/0Normal/1Fighting039_x264 normal normal\n",
      "sample_images_RA/0Normal/1Shooting002_x264 normal abnormal\n",
      "sample_images_RA/0Normal/1Shooting003_x264 normal abnormal\n",
      "sample_images_RA/0Normal/1Shooting004_x264 normal abnormal\n",
      "sample_images_RA/0Normal/1Vandalism019_x264 normal normal\n",
      "sample_images_RA/0Normal/1Vandalism020_x264 normal normal\n",
      "sample_images_RA/fighting/2Fighting015_x264 abnormal abnormal\n",
      "sample_images_RA/fighting/2Fighting016_x264 abnormal abnormal\n",
      "sample_images_RA/shooting/2Shooting038_x264 abnormal abnormal\n",
      "sample_images_RA/shooting/2Shooting053_x264 abnormal normal\n",
      "sample_images_RA/shooting/2Shooting054_x264 abnormal abnormal\n",
      "sample_images_RA/vandalism/2Vandalism036_x264 abnormal abnormal\n",
      "sample_images_RA/vandalism/2Vandalism037_x264 abnormal abnormal\n",
      "sample_images_RA/vandalism/2Vandalism038_x264 abnormal normal\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for i in Test_videos:\n",
    "    print(i, orignal[j], prediction[j])\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd5a702b-d553-4238-85b1-3099ee128cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(orignal, prediction)\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679188f-1fa0-456f-985a-f15a3c327757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d515d-2bfc-4617-81ea-d3f4c162c03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
