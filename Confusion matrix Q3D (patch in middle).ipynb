{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999d2031-a2e9-43b4-bfb4-47018a1495f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "orignal =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be5c30d-38a4-4a1c-b7ee-919f2ffefe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all test videos\n",
    "test=[]\n",
    "f= 'sample_images_RA'\n",
    "Test_videos =[]\n",
    "test_list = open('Labels/test.txt','r')\n",
    "for line in test_list:\n",
    "    name = line.split(\" \")[0]\n",
    "    x=line.split(\" \")[-1]\n",
    "    test.append(x[0])\n",
    "    Test_videos.append(f+\"/\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bd5e76e-36c8-4df1-ae53-edc11c52f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    if i=='0':\n",
    "        orignal.append('normal')\n",
    "    else:\n",
    "        orignal.append('abnormal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f6a471-4e68-46d8-917d-ad19e0352274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load pre-trained model, trained on 5 classes \n",
    "#model = tf.keras.models.load_model('Trained Models/model_P3D.h5',compile=False)\n",
    "\n",
    "\n",
    "#To load pre-trained weights,\n",
    "model = tf.keras.models.load_model('Trained Models/weights_model_Q3D.hdf5',compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e5225d1-9b25-4f00-b0b0-4cb667ac08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two classes labels\n",
    "labels0 ={0:'normal', 1:'abnormal'}\n",
    "prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4868d360-2e33-4708-95ef-6ed0eaea2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Patch image\n",
    "wm = cv2.imread(\"patch 3.jpg\")\n",
    "wm = cv2.resize(wm, (25, 25))\n",
    "wm =cv2.cvtColor(wm, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b5c5e02-4633-4f42-a487-d5916dea2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have defined 'Test_videos', 'wm', 'labels0', and 'model' elsewhere in your code\n",
    "\n",
    "prediction = []\n",
    "\n",
    "for video in Test_videos:\n",
    "    vid = []\n",
    "    nor = 0\n",
    "    abnor = 0\n",
    "    path = video\n",
    "    images = os.listdir(video)\n",
    "    \n",
    "    for img in images:\n",
    "        img_path = os.path.join(path, img)\n",
    "        img2 = cv2.imread(img_path)\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        img2 = cv2.resize(img2, (112, 112))\n",
    "        \n",
    "        # Extracting the central 25x25 patch\n",
    "        middle_patch = img2[43:68, 43:68]  # Adjust the indices based on your image size\n",
    "        \n",
    "        result = cv2.addWeighted(middle_patch, 1, wm, 1, 0)\n",
    "        img2[43:68, 43:68] = result\n",
    "        vid.append(img2)\n",
    "    \n",
    "    vid = np.array(vid, dtype=np.float32)\n",
    "    length = len(vid)\n",
    "    \n",
    "    for i in range(0, len(vid), 32):\n",
    "        if abnor == 3:\n",
    "            break\n",
    "        if i + 32 <= length:\n",
    "            X = vid[i:i + 32].transpose((1, 2, 0, 3))\n",
    "            output = model.predict_on_batch(np.array([X]))\n",
    "            \n",
    "            if len(output) > 0:\n",
    "                str1 = labels0[np.argmax(output)]\n",
    "                \n",
    "                if str1 == 'abnormal':\n",
    "                    abnor += 1\n",
    "                else:\n",
    "                    abnor = 0\n",
    "    \n",
    "    vid = []\n",
    "    \n",
    "    if abnor == 3:\n",
    "        prediction.append('abnormal')\n",
    "    else:\n",
    "        prediction.append('normal')\n",
    "\n",
    "# Now 'prediction' contains the predictions for each video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b0dee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have defined 'Test_videos', 'wm', 'labels0', and 'model' elsewhere in your code\n",
    "\n",
    "prediction = []\n",
    "\n",
    "for video in Test_videos:\n",
    "    vid = []\n",
    "    nor = 0\n",
    "    abnor = 0\n",
    "    path = video\n",
    "    images = os.listdir(video)\n",
    "    \n",
    "    for img in images:\n",
    "        img_path = os.path.join(path, img)\n",
    "        img2 = cv2.imread(img_path)\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        img2 = cv2.resize(img2, (112, 112))\n",
    "        \n",
    "        # Extracting the central 25x25 patch\n",
    "        middle_patch = img2[43:68, 43:68]  # Adjust the indices based on your image size\n",
    "        \n",
    "        result = cv2.addWeighted(middle_patch, 1, wm, 1, 0)\n",
    "        img2[43:68, 43:68] = result\n",
    "        vid.append(img2)\n",
    "    \n",
    "    vid = np.array(vid, dtype=np.float32)\n",
    "    length = len(vid)\n",
    "    \n",
    "    for i in range(0, len(vid), 32):\n",
    "        if abnor == 3:\n",
    "            break\n",
    "        if i + 32 <= length:\n",
    "            X = vid[i:i + 32].transpose((1, 2, 0, 3))\n",
    "            output = model.predict_on_batch(np.array([X]))\n",
    "            \n",
    "            if len(output) > 0:\n",
    "                str1 = labels0[np.argmax(output)]\n",
    "                \n",
    "                if str1 == 'abnormal':\n",
    "                    abnor += 1\n",
    "                else:\n",
    "                    abnor = 0\n",
    "    \n",
    "    vid = []\n",
    "    \n",
    "    if abnor == 3:\n",
    "        prediction.append('abnormal')\n",
    "    else:\n",
    "        prediction.append('normal')\n",
    "\n",
    "# Now 'prediction' contains the predictions for each video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfa9141f-755d-424f-a611-d1fa7d260253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33efa50d-2140-4b57-90bb-cdfa786ed0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7799054c-440a-4f5a-8997-200c830f83ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_images_RA/0Normal/1Fighting037_x264 normal abnormal\n",
      "sample_images_RA/0Normal/1Fighting038_x264 normal normal\n",
      "sample_images_RA/0Normal/1Fighting039_x264 normal abnormal\n",
      "sample_images_RA/0Normal/1Shooting002_x264 normal abnormal\n",
      "sample_images_RA/0Normal/1Shooting003_x264 normal abnormal\n",
      "sample_images_RA/0Normal/1Shooting004_x264 normal abnormal\n",
      "sample_images_RA/0Normal/1Vandalism019_x264 normal abnormal\n",
      "sample_images_RA/0Normal/1Vandalism020_x264 normal abnormal\n",
      "sample_images_RA/fighting/2Fighting015_x264 abnormal abnormal\n",
      "sample_images_RA/fighting/2Fighting016_x264 abnormal abnormal\n",
      "sample_images_RA/shooting/2Shooting038_x264 abnormal abnormal\n",
      "sample_images_RA/shooting/2Shooting053_x264 abnormal abnormal\n",
      "sample_images_RA/shooting/2Shooting054_x264 abnormal abnormal\n",
      "sample_images_RA/vandalism/2Vandalism036_x264 abnormal abnormal\n",
      "sample_images_RA/vandalism/2Vandalism037_x264 abnormal abnormal\n",
      "sample_images_RA/vandalism/2Vandalism038_x264 abnormal abnormal\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for i in Test_videos:\n",
    "    print(i, orignal[j], prediction[j])\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd5a702b-d553-4238-85b1-3099ee128cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(orignal, prediction)\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2201870-6944-487a-b10d-733ff1b78066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
